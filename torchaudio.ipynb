{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ifire/.local/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/ifire/.local/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "import torchaudio\n",
    "import torch\n",
    "from dataloader import ESC50\n",
    "from pathlib import Path\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "import torchvision\n",
    "from matplotlib import pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_esc = ESC50(\"/home/ifire/learnpytorh/audio/ESC-50/audio\")\n",
    "tensor, label = list(test_esc)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor.shape, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_dataloader = torch.utils.data.DataLoader(test_esc, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Path.cwd() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_ESC50 = Path.cwd() / 'ESC-50/audio'\n",
    "PATH_TO_ESC50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_es50 = ESC50(PATH_TO_ESC50/ 'train')\n",
    "valid_es50 = ESC50(PATH_TO_ESC50/ 'valid')\n",
    "test_es50 = ESC50(PATH_TO_ESC50/ 'test')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_es50, batch_size=BATCH_SIZE, shuffle=True)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_es50, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_es50, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## core model \n",
    "class AudioNet(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super(AudioNet, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(1,128,80,4)\n",
    "        self.bn1 = nn.BatchNorm1d(128)\n",
    "        self.pool1 = nn.MaxPool1d(4)\n",
    "        self.conv2 = nn.Conv1d(128,128,80,3)\n",
    "        self.bn2 = nn.BatchNorm1d(128)\n",
    "        self.pool2 = nn.MaxPool1d(4)\n",
    "        self.conv3 = nn.Conv1d(128,256,80,3)\n",
    "        self.bn3 = nn.BatchNorm1d(256)\n",
    "        self.pool3 = nn.MaxPool1d(4)\n",
    "        self.conv4 = nn.Conv1d(256,512,80,3)\n",
    "        self.bn4 = nn.BatchNorm1d(512)\n",
    "        self.pool4 = nn.MaxPool1d(4)\n",
    "        self.avgpool = nn.AvgPool1d(30)\n",
    "        self.fc1 = nn.Linear(512, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(self.bn1(x))\n",
    "        x = self.pool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x =  F.relu(self.bn2(x))\n",
    "        x = self.pool2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = F.relu(self.bn3(x))\n",
    "        x = self.pool3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = F.relu(self.bn4(x))\n",
    "        x = self.pool4(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = x.permute(0,2,1)\n",
    "        x = self.fc1(x)\n",
    "        \n",
    "        return F.log_softmax(x, dim=2)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_net = AudioNet()\n",
    "audio_net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## model hyper parameters\n",
    "\n",
    "optimizer = optim.Adam(audio_net.parameters(), lr = 1e-5)\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(10):\n",
    "    audio_net.to(device)\n",
    "    audio_net.train()\n",
    "    for out in enumerate(train_loader):\n",
    "        print(len(out[1]))\n",
    "        input, label = input.to(device), label.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = audio_net(input)\n",
    "        loss = loss_fn(output, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data, sr = librosa.load(\"/home/ifire/learnpytorh/audio/ESC-50/audio/train/1-100032-A-0.wav\",sr=None)\n",
    "spectrogram = librosa.feature.melspectrogram(y=sample_data, sr=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "librosa.display.specshow(spectrogram, sr=sr, x_axis='time', y_axis='mel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lof_spectogram = librosa.power_to_db(spectrogram, ref=np.max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "librosa.display.specshow(lof_spectogram, sr=sr, x_axis='time', y_axis='mel')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "because these spectograms are image data then it contains the structure of the audio data\n",
    "therefore we can use the spectgrams along with CCN2d to capture the sturcture and classify them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## new data set with spectograms\n",
    "class ESC50Spectograms(Dataset):\n",
    "    def __init__(self, path) -> None:\n",
    "        files = Path(path).glob('*.wav')\n",
    "        self.items = [(f, int(f.name.split(\"-\")[-1].replace(\".wav\", \"\"))) for f in files]\n",
    "        self.length = len(self.items)\n",
    "        self.tranforms = torchvision.transforms.Compose([torchvision.transforms.ToTensor()])\n",
    "        \n",
    "    def __getitem__(self, index) -> torch.Tensor:\n",
    "        filename, label = self.items[index]\n",
    "        audio_tensor, sample_rate = librosa.load(filename, sr=None)\n",
    "        spectrogram = librosa.feature.melspectrogram(y=audio_tensor, sr= sample_rate)\n",
    "        lof_spectogram = librosa.power_to_db(spectrogram, ref=np.max)\n",
    "        librosa.display.specshow(lof_spectogram, sr=sample_rate, x_axis='time', y_axis='mel')\n",
    "        \n",
    "        plt.gcf().canvas.draw()\n",
    "        audio_data = np.frombuffer(fig.canvas.tostring_rgb(), dtype=np.uint8)\n",
    "        audio_data = audio_data.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n",
    "        return (self.transforms(audio_data), label)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oldESC50  = ESC50(PATH_TO_ESC50/ 'train')\n",
    "start_time = time.process_time()\n",
    "oldESC50.__getitem__(33)\n",
    "end_time = time.process_time()\n",
    "old_time = end_time - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newESC50 = ESC50Spectograms(PATH_TO_ESC50/ 'train')\n",
    "start_time = time.process_time()\n",
    "newESC50.__getitem__(33)\n",
    "end_time = time.process_time()\n",
    "new_time = end_time - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter()\n",
    "writer.add_scalar('example', 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## testing with random values\n",
    "for i in range(100):\n",
    "    writer.add_scalar('Loss', np.random.random(), i)\n",
    "    writer.add_scalar('Accuracy', np.random.random(), i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## model representation \n",
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision import datasets, transforms, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SummaryWriter' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m writer \u001b[38;5;241m=\u001b[39m \u001b[43mSummaryWriter\u001b[49m()\n\u001b[1;32m      2\u001b[0m model \u001b[38;5;241m=\u001b[39m models\u001b[38;5;241m.\u001b[39mresnet18(\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m      3\u001b[0m writer\u001b[38;5;241m.\u001b[39madd_graph(model, torch\u001b[38;5;241m.\u001b[39mrand(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m224\u001b[39m,\u001b[38;5;241m224\u001b[39m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'SummaryWriter' is not defined"
     ]
    }
   ],
   "source": [
    "writer = SummaryWriter()\n",
    "model = models.resnet18(False)\n",
    "writer.add_graph(model, torch.rand(1,3,224,224))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,\n",
    "          optimizer,\n",
    "          loss_fn,\n",
    "          train_loader,\n",
    "          test_loader,\n",
    "          epochs=50):\n",
    "    model = model.train()\n",
    "    iteration = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.to(device)\n",
    "        model.train()\n",
    "        for batch in train_loader:\n",
    "            input, label = batch\n",
    "            optimizer.zero_grad()\n",
    "            input, label = input.to(device), label.to(device)\n",
    "            output = model(input)\n",
    "            loss = loss_fn(output, label)\n",
    "            \n",
    "            writer.add_scalar('loss', loss, epoch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        model.eval()\n",
    "        num_corrects = 0\n",
    "        num_example = 0\n",
    "        for batch in test_loader:\n",
    "            input, label = batch\n",
    "            input, label = input.to(device), label.to(device)\n",
    "            output = model(input)\n",
    "            correct = torch.eq(torch.max(F.softmax(output), dim=1)[1], label).view(-1)\n",
    "            \n",
    "            num_corrects += torch.sum(correct).item()\n",
    "            num_example += correct.shape[0]\n",
    "            \n",
    "            print('[INFO] Epoch - {epoch} : accuracy - {num_corrects/num_example}')\n",
    "        iteration +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_hook(self, module, input):\n",
    "    print(f\"Shape of the input is - {input.shape} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'models' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mmodels\u001b[49m\u001b[38;5;241m.\u001b[39mresnet18()\n\u001b[1;32m      2\u001b[0m hook_ref \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfc\u001b[38;5;241m.\u001b[39mregister_forward_hook(print_hook)\n\u001b[1;32m      3\u001b[0m model(torch\u001b[38;5;241m.\u001b[39mrand([\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m224\u001b[39m,\u001b[38;5;241m224\u001b[39m]))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'models' is not defined"
     ]
    }
   ],
   "source": [
    "model = models.resnet18()\n",
    "hook_ref = model.fc.register_forward_hook(print_hook)\n",
    "model(torch.rand([1,3,224,224]))\n",
    "hook_ref.remove()\n",
    "model(torch.rand([1,3,224,224]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sen_to_tensorboard(i, module, input, output):\n",
    "    writer.add_scalar(f\"{i}-mean \", output.data.mean())\n",
    "    writer.add_scalar(f\"{i}-stddev \", output.data.std())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.children"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, m in enumerate(model.children()):\n",
    "    print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "for i, m in enumerate(model.children()):\n",
    "    m.register_forward_hook(partial(sen_to_tensorboard, i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = datasets.MNIST(root = \".\", train=True, transform=transforms.ToTensor(), download=True)\n",
    "test_data = datasets.MNIST(root = \".\", train=False, transform=transforms.ToTensor(), download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils\n",
    "from  torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(train_data, batch_size=64, shuffle=True, num_workers=5, drop_last=True, pin_memory=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64, shuffle=False, num_workers=5, drop_last=True, pin_memory=True)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 3e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "\n",
    "    \n",
    "input_shape = model.fc.in_features\n",
    "hidden_units = 128\n",
    "model.fc = nn.Sequential(\n",
    "    nn.Linear(input_shape, hidden_units),\n",
    "    nn.BatchNorm1d(hidden_units),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.5),\n",
    "    nn.Linear(hidden_units, 10),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(model=model, \n",
    "      optimizer=optimizer,\n",
    "      loss_fn=loss_fn,\n",
    "      train_loader=train_dataloader, \n",
    "      test_loader=test_dataloader,\n",
    "      epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Saveactivations():\n",
    "    acvtivations = None\n",
    "    def __init__(self, m):\n",
    "        self.hook = m.register_forward_hook(self.hook_fn)\n",
    "    \n",
    "    def hook_fn(self, module, input, output):\n",
    "        self.features = output.data\n",
    "    \n",
    "    def remove(self):\n",
    "        self.hook.remove() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "from torch.nn import functional as F\n",
    "from torchvision import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat = Image.open('pexels-photo-1170986.png')\n",
    "##then we normalize\n",
    "normalize = transforms.Normalize(mean  = [0.4914, 0.4822, 0.4465], std=[0.2023, 0.1994, 0.2010])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.resnet18()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "\n",
    "    \n",
    "input_shape = model.fc.in_features\n",
    "hidden_units = 128\n",
    "model.fc = nn.Sequential(\n",
    "    nn.Linear(input_shape, hidden_units),\n",
    "    nn.BatchNorm1d(hidden_units),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.5),\n",
    "    nn.Linear(hidden_units, 10),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = transforms.Compose([transforms.Resize((224,224)),\n",
    "                                 transforms.ToTensor(),\n",
    "                                 normalize])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_transforms = transforms.Compose([transforms.Resize((224, 224))])\n",
    "casper_tensor = preprocess(cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1716464/1064327656.py:5: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pred_probabilities  = F.softmax(prediction).data.squeeze()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.return_types.topk(\n",
       "values=tensor([0.2894]),\n",
       "indices=tensor([285]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = models.resnet18(pretrained=True)\n",
    "model.eval()\n",
    "casper_activations = Saveactivations(model.layer4)\n",
    "prediction = model(casper_tensor.unsqueeze(0))\n",
    "pred_probabilities  = F.softmax(prediction).data.squeeze()\n",
    "casper_activations.remove()\n",
    "torch.topk(pred_probabilities,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(casper_activations.acvtivations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SaveActivations():\n",
    "    activations=None\n",
    "    def __init__(self, m):\n",
    "        self.hook = m.register_forward_hook(self.hook_fn)\n",
    "    def hook_fn(self, module, input, output):\n",
    "        self.features = output.data\n",
    "    def remove(self):\n",
    "        self.hook.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import models, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "casper = Image.open(\"pexels-photo-1170986.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize = transforms.Normalize(\n",
    " mean=[0.485, 0.456, 0.406],\n",
    " std=[0.229, 0.224, 0.225]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = transforms.Compose([\n",
    " transforms.Resize((224,224)),\n",
    " transforms.ToTensor(),\n",
    " normalize\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_transform = transforms.Compose([\n",
    " transforms.Resize((224,224))])\n",
    "casper_tensor = preprocess(casper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ifire/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/ifire/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "/tmp/ipykernel_1716464/1944733709.py:5: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pred_probabilities = F.softmax(prediction).data.squeeze()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.return_types.topk(\n",
       "values=tensor([0.2744]),\n",
       "indices=tensor([281]))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = models.resnet18(pretrained=True)\n",
    "model.eval()\n",
    "casper_activations = SaveActivations(model.layer4)\n",
    "prediction = model(casper_tensor.unsqueeze(0))\n",
    "pred_probabilities = F.softmax(prediction).data.squeeze()\n",
    "casper_activations.remove()\n",
    "torch.topk(pred_probabilities,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (378078884.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[41], line 2\u001b[0;36m\u001b[0m\n\u001b[0;31m    prob = np.exp(to_np(log_prob))\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "fts = sf[0].features[idx]\n",
    "    prob = np.exp(to_np(log_prob))\n",
    "    preds = np.argmax(prob[idx])\n",
    "    fts_np = to_np(fts)\n",
    "    f2=np.dot(np.rollaxis(fts_np,0,3), prob[idx])\n",
    "    f2-=f2.min()\n",
    "    f2/=f2.max()\n",
    "    f2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
